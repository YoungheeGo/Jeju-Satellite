{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
        "from math import sqrt\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv('/content/sst_data.csv')\n",
        "\n",
        "# 위도와 경도를 기반으로 데이터 그룹화 및 평균화\n",
        "df_grouped = df.groupby(['latitude', 'longitude', 'time']).mean().reset_index()"
      ],
      "metadata": {
        "id": "QtfUZqJCVotM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 스케일링\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "df_grouped['scaled_sst'] = scaler.fit_transform(df_grouped['sst'].values.reshape(-1,1))\n",
        "\n",
        "# 학습 데이터셋 생성\n",
        "def create_dataset(df, seq_len):\n",
        "    data = []\n",
        "    for i in range(len(df)-seq_len):\n",
        "        data.append(df[i:i+seq_len])\n",
        "    return np.array(data)\n",
        "\n",
        "seq_len = 15\n",
        "data = create_dataset(df_grouped['scaled_sst'], seq_len)\n",
        "train_set_size = int(np.round(0.9*data.shape[0]))\n",
        "train_set = data[:train_set_size]\n",
        "test_set = data[train_set_size:]\n",
        "\n",
        "X_train = train_set[:,:-1]\n",
        "y_train = train_set[:,-1]\n",
        "X_test = test_set[:,:-1]\n",
        "y_test = test_set[:,-1]\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "PU0BL5nxVx8h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(50, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Adam 옵티마이저 생성\n",
        "adam = Adam(learning_rate=0.01)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XwEHmUIVyiW",
        "outputId": "0d619be5-b4e2-401a-a268-06c62896a817"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1368/1368 [==============================] - 45s 31ms/step - loss: 0.0026\n",
            "Epoch 2/100\n",
            "1368/1368 [==============================] - 48s 35ms/step - loss: 8.9233e-04\n",
            "Epoch 3/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 8.6061e-04\n",
            "Epoch 4/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 8.3700e-04\n",
            "Epoch 5/100\n",
            "1368/1368 [==============================] - 44s 33ms/step - loss: 8.4823e-04\n",
            "Epoch 6/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 8.2621e-04\n",
            "Epoch 7/100\n",
            "1368/1368 [==============================] - 42s 31ms/step - loss: 8.2580e-04\n",
            "Epoch 8/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 8.0312e-04\n",
            "Epoch 9/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 8.1716e-04\n",
            "Epoch 10/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.9668e-04\n",
            "Epoch 11/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.8999e-04\n",
            "Epoch 12/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.8977e-04\n",
            "Epoch 13/100\n",
            "1368/1368 [==============================] - 44s 33ms/step - loss: 7.7917e-04\n",
            "Epoch 14/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.7126e-04\n",
            "Epoch 15/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.7031e-04\n",
            "Epoch 16/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.7780e-04\n",
            "Epoch 17/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.6758e-04\n",
            "Epoch 18/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.6647e-04\n",
            "Epoch 19/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 0.0610\n",
            "Epoch 20/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.6270e-04\n",
            "Epoch 21/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.5789e-04\n",
            "Epoch 22/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.5502e-04\n",
            "Epoch 23/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.5903e-04\n",
            "Epoch 24/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.5823e-04\n",
            "Epoch 25/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.5538e-04\n",
            "Epoch 26/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.5763e-04\n",
            "Epoch 27/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.8176e-04\n",
            "Epoch 28/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.5709e-04\n",
            "Epoch 29/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.5519e-04\n",
            "Epoch 30/100\n",
            "1368/1368 [==============================] - 42s 31ms/step - loss: 7.5553e-04\n",
            "Epoch 31/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.6743e-04\n",
            "Epoch 32/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.7163e-04\n",
            "Epoch 33/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.6528e-04\n",
            "Epoch 34/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.5885e-04\n",
            "Epoch 35/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.5960e-04\n",
            "Epoch 36/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.6039e-04\n",
            "Epoch 37/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 8.8791e-04\n",
            "Epoch 38/100\n",
            "1368/1368 [==============================] - 42s 31ms/step - loss: 7.5260e-04\n",
            "Epoch 39/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4338e-04\n",
            "Epoch 40/100\n",
            "1368/1368 [==============================] - 44s 33ms/step - loss: 7.4478e-04\n",
            "Epoch 41/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4567e-04\n",
            "Epoch 42/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.4411e-04\n",
            "Epoch 43/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4567e-04\n",
            "Epoch 44/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.5217e-04\n",
            "Epoch 45/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.4779e-04\n",
            "Epoch 46/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.5224e-04\n",
            "Epoch 47/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.4942e-04\n",
            "Epoch 48/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4683e-04\n",
            "Epoch 49/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4815e-04\n",
            "Epoch 50/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4359e-04\n",
            "Epoch 51/100\n",
            "1368/1368 [==============================] - 42s 31ms/step - loss: 7.5041e-04\n",
            "Epoch 52/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.4376e-04\n",
            "Epoch 53/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3879e-04\n",
            "Epoch 54/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.4217e-04\n",
            "Epoch 55/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3604e-04\n",
            "Epoch 56/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.4039e-04\n",
            "Epoch 57/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3333e-04\n",
            "Epoch 58/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3828e-04\n",
            "Epoch 59/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 2.4588\n",
            "Epoch 60/100\n",
            "1368/1368 [==============================] - 42s 31ms/step - loss: 7.6106e-04\n",
            "Epoch 61/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3944e-04\n",
            "Epoch 62/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3106e-04\n",
            "Epoch 63/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.2935e-04\n",
            "Epoch 64/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.2740e-04\n",
            "Epoch 65/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.2978e-04\n",
            "Epoch 66/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.2937e-04\n",
            "Epoch 67/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3251e-04\n",
            "Epoch 68/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3416e-04\n",
            "Epoch 69/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3409e-04\n",
            "Epoch 70/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.3162e-04\n",
            "Epoch 71/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3817e-04\n",
            "Epoch 72/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3722e-04\n",
            "Epoch 73/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3571e-04\n",
            "Epoch 74/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.4258e-04\n",
            "Epoch 75/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3424e-04\n",
            "Epoch 76/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.4012e-04\n",
            "Epoch 77/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3723e-04\n",
            "Epoch 78/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.3388e-04\n",
            "Epoch 79/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3252e-04\n",
            "Epoch 80/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3715e-04\n",
            "Epoch 81/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 7.3597e-04\n",
            "Epoch 82/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 791430.8125\n",
            "Epoch 83/100\n",
            "1368/1368 [==============================] - 41s 30ms/step - loss: 7.6375e-04\n",
            "Epoch 84/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.5107e-04\n",
            "Epoch 85/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.4839e-04\n",
            "Epoch 86/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.4217e-04\n",
            "Epoch 87/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.3850e-04\n",
            "Epoch 88/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3782e-04\n",
            "Epoch 89/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3592e-04\n",
            "Epoch 90/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.3496e-04\n",
            "Epoch 91/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.2964e-04\n",
            "Epoch 92/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 230776.6094\n",
            "Epoch 93/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 8.2711e-04\n",
            "Epoch 94/100\n",
            "1368/1368 [==============================] - 45s 33ms/step - loss: 8.0279e-04\n",
            "Epoch 95/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.9067e-04\n",
            "Epoch 96/100\n",
            "1368/1368 [==============================] - 43s 32ms/step - loss: 7.9229e-04\n",
            "Epoch 97/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.8011e-04\n",
            "Epoch 98/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.8539e-04\n",
            "Epoch 99/100\n",
            "1368/1368 [==============================] - 44s 32ms/step - loss: 7.7897e-04\n",
            "Epoch 100/100\n",
            "1368/1368 [==============================] - 43s 31ms/step - loss: 7.7673e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7adfecec7e80>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# 스케일링 되돌리기\n",
        "y_train_inv = scaler.inverse_transform(y_train.reshape(1, -1))\n",
        "y_pred_train_inv = scaler.inverse_transform(y_pred_train)\n",
        "y_test_inv = scaler.inverse_transform(y_test.reshape(1, -1))\n",
        "y_pred_test_inv = scaler.inverse_transform(y_pred_test)\n",
        "\n",
        "# 평가 지표 계산\n",
        "train_rmse = sqrt(mean_squared_error(y_train_inv.flatten(), y_pred_train_inv.flatten()))\n",
        "test_rmse = sqrt(mean_squared_error(y_test_inv.flatten(), y_pred_test_inv.flatten()))\n",
        "\n",
        "train_r2 = r2_score(y_train_inv.flatten(), y_pred_train_inv.flatten())\n",
        "test_r2 = r2_score(y_test_inv.flatten(), y_pred_test_inv.flatten())\n",
        "\n",
        "train_mape = mean_absolute_percentage_error(y_train_inv.flatten(), y_pred_train_inv.flatten())\n",
        "test_mape = mean_absolute_percentage_error(y_test_inv.flatten(), y_pred_test_inv.flatten())\n",
        "\n",
        "print('Train RMSE: ', train_rmse)\n",
        "print('Test RMSE: ', test_rmse)\n",
        "\n",
        "print('Train R^2 Score: ', train_r2)\n",
        "print('Test R^2 Score: ', test_r2)\n",
        "\n",
        "print('Train MAPE: ', train_mape)\n",
        "print('Test MAPE: ', test_mape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_FCFIHvVyqw",
        "outputId": "e6c8ba06-70ed-443d-c4f8-85a489e34ab9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2736/2736 [==============================] - 20s 7ms/step\n",
            "304/304 [==============================] - 2s 8ms/step\n",
            "Train RMSE:  0.32000464455083266\n",
            "Test RMSE:  0.3374520311539128\n",
            "Train R^2 Score:  0.9957097815653033\n",
            "Test R^2 Score:  0.9942776913648289\n",
            "Train MAPE:  0.011623423149717153\n",
            "Test MAPE:  0.012519059993342095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'time' 열을 datetime 형태로 변환\n",
        "df_grouped['time'] = pd.to_datetime(df_grouped['time'])\n",
        "\n",
        "# 2023년 7월부터 9월까지의 데이터 선택\n",
        "df_2023_summer = df_grouped[(df_grouped['time'].dt.year == 2023) & (df_grouped['time'].dt.month >= 7) & (df_grouped['time'].dt.month <= 9)]\n",
        "\n",
        "# 'sst' 데이터 스케일링\n",
        "df_2023_summer['scaled_sst'] = scaler.transform(df_2023_summer['sst'].values.reshape(-1,1))\n",
        "\n",
        "# 데이터셋 생성\n",
        "data_2023_summer = create_dataset(df_2023_summer['scaled_sst'], seq_len)\n",
        "\n",
        "# 데이터 형태 변환\n",
        "X_2023_summer = np.reshape(data_2023_summer, (data_2023_summer.shape[0], data_2023_summer.shape[1], 1))\n",
        "\n",
        "# 예측\n",
        "y_pred_2023_summer = model.predict(X_2023_summer)\n",
        "\n",
        "# 스케일링 되돌리기\n",
        "y_pred_2023_summer_inv = scaler.inverse_transform(y_pred_2023_summer)\n",
        "\n",
        "# 실제 값 추출\n",
        "y_true_2023_summer = df_2023_summer['sst'].values[seq_len:]\n",
        "\n",
        "# 평가 지표 계산\n",
        "rmse = sqrt(mean_squared_error(y_true_2023_summer, y_pred_2023_summer_inv.flatten()))\n",
        "r2 = r2_score(y_true_2023_summer, y_pred_2023_summer_inv.flatten())\n",
        "mape = mean_absolute_percentage_error(y_true_2023_summer, y_pred_2023_summer_inv.flatten())\n",
        "\n",
        "# 결과 출력\n",
        "print('RMSE: ', rmse)\n",
        "print('R^2 Score: ', r2)\n",
        "print('MAPE: ', mape)\n",
        "\n",
        "# 예측 결과를 DataFrame으로 변환\n",
        "df_result = pd.DataFrame(y_pred_2023_summer_inv, columns=['Predicted_SST'])\n",
        "\n",
        "# 'time', 'latitude', 'longitude' 열 추가\n",
        "df_result['time'] = df_2023_summer['time'].values[seq_len:]\n",
        "df_result['latitude'] = df_2023_summer['latitude'].values[seq_len:]\n",
        "df_result['longitude'] = df_2023_summer['longitude'].values[seq_len:]\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "df_result.to_csv('2023_summer_sst_prediction.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGMe1b_sqEHq",
        "outputId": "c4c4bbb5-782b-45f5-9978-827533e030a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6adb592079ae>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_2023_summer['scaled_sst'] = scaler.transform(df_2023_summer['sst'].values.reshape(-1,1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98/98 [==============================] - 1s 8ms/step\n",
            "RMSE:  0.42062118939626403\n",
            "R^2 Score:  0.939473367042511\n",
            "MAPE:  0.010449106097144832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rktT68P8s-ZA",
        "outputId": "b1c4b502-908a-4f02-9c94-29b5a8ce769c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}